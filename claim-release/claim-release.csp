include "variables.csp"
include "conc-queue.csp"
include "mutex-spec.csp"

-- Processes used by the system
datatype PID = NULL | P1 | P2 | P3 | P4
subtype NonNullPID = P1 | P2 | P3 | P4

-- Users being applied in the model for assertion checking.
users = {P1, P2, P3, P4}

-- Channels to interact with the queue
channel enqueue : NonNullPID.NonNullPID
channel end_enqueue : NonNullPID
channel dequeue : NonNullPID
channel return : NonNullPID.PID
channel peek : NonNullPID

-- NOT READY and NOT RUNNING - waiting
-- NOT READY and RUNNING - engaging
-- READY and NOT RUNNING - scheduled
-- READY and RUNNING - active
datatype STATE = WAITING | ENGAGING | SCHEDULED | ACTIVE
channel state : Operations.NonNullPID.STATE
channel cas_state : NonNullPID.STATE.STATE.Bool

STATE_VAR =
    ||| p : NonNullPID @ ATOMIC_VARIABLE(state.load.p, state.store.p, cas_state.p, ACTIVE)

getState = state.load
setState = state.store
casState = cas_state

-- Instance of the concurrent queue
instance Q = ConcQueue(card(users))

-- YIELD just waits until scheduled. As only scheduled processes are runnable
-- this is a sufficient model. True scheduling is more restrictive.

YIELD(pid) = 
    getState.pid.SCHEDULED -> 
    setState.pid.ACTIVE -> 
    SKIP

-- SCHEDULE sets the state of a process to scheduled. A process may be engaging
-- (attempting to enter a critical section) or waiting (waiting to enter)
SCHEDULE(pid) = 
    casState.pid!ENGAGING!SCHEDULED?succ ->
    if (not succ) then
        casState.pid!WAITING!SCHEDULED?succ ->
        SKIP
    else
        SKIP

-- Channels to access the atomic variable owner.
-- This could be an atomic reference in Java (i.e., object is stored in the
-- shared mutex), or could be an ID for lookup. The algorithm does not
-- distinguish and uses the owner ID to lookup a ready flag index.
channel owner : Operations.PID
channel owner_cas : PID.PID.Bool

-- Helper aliases to use the owner channels
getOwner = owner.load
setOwner = owner.store
casOwner = owner_cas

-- Process representing the atomic variable of the owner
OWNER = ATOMIC_VARIABLE(owner.load, owner.store, casOwner, NULL)

-- Channels used to interact with the shared object
channel begin_claim : NonNullPID
channel end_claim : NonNullPID
channel begin_release : NonNullPID
channel end_release : NonNullPID

-- Claim process
CLAIM(pid) =
    begin_claim.pid ->
    -- Set ourselves engaging
    setState.pid!ENGAGING ->
    -- Enqueue ourselves
    enqueue.pid.pid ->
    end_enqueue.pid ->
    -- Check who is head of the queue
    peek.pid ->
    return.pid?headQ ->
    if headQ == pid then
        -- We know we are head of the queue. But is another
        -- process releasing?
        -- Let's see if owner is NULL
        casOwner!NULL!pid?succ ->
        if succ then
            -- Owner is NULL. Release does this last, so
            -- nothing in our way. Set ourselves active
            setState.pid!ACTIVE ->
            end_claim.pid ->
            SKIP
        else
            -- OK, at the cas point RELEASE was in only a few
            -- places. We know it has dequeued at least.
            -- We can check the owner. But might have changed.
            getOwner?own ->
            -- Check if now null
            if own == NULL then
                setOwner!pid ->
                setState.pid!ACTIVE ->
                end_claim.pid -> 
                SKIP
            -- Are we the owner?
            else if own == pid then
                -- Check if we are scheduled
                casState.pid!ENGAGING!WAITING?succ ->
                if succ then
                    -- We wait for the schedule signal from release.
                    YIELD(pid);
                    end_claim.pid ->
                    SKIP
                else
                    -- Scheduled, set active
                    setState.pid!ACTIVE ->
                    end_claim.pid ->
                    SKIP
            else
                -- Owner was still there. Attempt steal
                casOwner!own!pid?succ ->
                if succ then
                    -- Stolen. Set active
                    setState.pid!ACTIVE ->
                    end_claim.pid ->
                    SKIP
                else
                    -- Owner must have changed. Test null first.
                    casOwner!NULL!pid?succ ->
                    if succ then
                        -- We now know that the owner was NULL
                        -- We can set ourselves active.
                        setState.pid!ACTIVE ->
                        end_claim.pid ->
                        SKIP
                    else
                        -- Owner must be us. Are we signalled?
                        casState.pid!ENGAGING!WAITING?succ ->
                        if succ then
                            -- We wait for the schedule signal from release.
                            YIELD(pid);
                            end_claim.pid ->
                            SKIP
                        else
                            -- Scheduled, set active
                            setState.pid!ACTIVE ->
                            end_claim.pid ->
                            SKIP           
    else
        -- OK, we are not the head of the queue. Something
        -- will wake us up.
        -- Attempt to CAS the state to waiting (may have been changed by release)
        casState.pid!ENGAGING!WAITING?succ ->
        if (succ) then
            -- Release didn't change state
            YIELD(pid);
            end_claim.pid ->
            SKIP
        else
            -- Release did change state. Can end now.
            setState.pid!ACTIVE ->
            end_claim.pid ->
            SKIP

-- Release process
RELEASE(pid) =
    begin_release.pid ->
    -- Remove ourselves from the queue
    dequeue.pid ->
    -- This will block if we are not at the head
    return.pid.pid ->
    -- We now check if someone else is in the queue
    peek.pid ->
    return.pid?headQ ->
    if headQ == NULL then
        -- Nothing in the queue
        -- Try to set owner to NULL
        casOwner!pid!NULL?succ ->
        -- Doesn't matter if we succeeded. Either we did, and another process
        -- can claim, or we didn't and another process has stolen.
        end_release.pid ->
        SKIP
    else
        -- Something is at the front of the queue.
        -- It has maybe been there for a while. Set the new
        -- owner.
        -- Try and set the new owner
        casOwner!pid!headQ?succ ->
        if succ then
            -- We successfully set it as the new owner.
            -- We must set it ready
            SCHEDULE(headQ);
            -- And we can end.
            end_release.pid ->
            SKIP
        else
            -- It must have set itself as the owner. We can 
            -- just end.
            end_release.pid ->
            SKIP

USER(pid) =
    CLAIM(pid);
    RELEASE(pid);
    USER(pid)

-- Users will use all the variables.
alphaVAR =
{|
    getState,
    setState,
    casState,
    getOwner,
    setOwner,
    casOwner
|}

-- System plugs the queue, state, and owner variables into the set of user
-- processes. Variables interleave and users interleave. Synchronisation is
-- between the variables and users.
-- We also hide the variables for verification checking.
SYSTEM =
(
    (Q::QUEUE(users) ||| STATE_VAR ||| OWNER)
    [| union(Q::alpha, alphaVAR) |]
    (||| u : users @ USER(u))
) \ union(Q::alpha, alphaVAR)

-- Can now perform basic checks
-- Is the shared object deadlock free? This ensures it does not get into a 
-- condition where waiting processes are not correctly notified.
assert SYSTEM :[deadlock free[F]]

-- Is the shared object divergence free? This ensures there is no spinning 
-- (i.e., spin lock) happening.
assert SYSTEM :[divergence free]

-- Is the system deterministic? It isn't. But neither is the spec.
-- This is due to not knowing the outcome of the internal race
-- to claim and claim-release. The steady state reached depends on 
-- internal interleaving.
assert not SYSTEM :[deterministic[FD]]

-- Create an instance of the specification (a mutex)
instance MUTEX = Mutex(users)
SPEC = MUTEX::SPEC

-- Ensure spec is also deadlock and divergence free
assert SPEC :[deadlock free[F]]
assert SPEC :[divergence free]

-- Also check determinism. See comment on SYSTEM determinism.
assert not SPEC :[deterministic [FD]]

-- As the system is divergence free, we only need to check in failures.
-- Check if the system only behaves as allowed by the specification.
assert SPEC [F= SYSTEM

-- Does the specification only behave as the implementation? If it does,
-- the external behaviour of the SPEC and SYSTEM are identical
assert SYSTEM [F= SPEC

-- As SPEC [F= SYSTEM and SYSTEM [F= SPEC, we have equivalent behaviour.
-- The CLAIM-RELEASE protocol works exactly like a shared mutex.

-- Linearizability checks
channel lock : NonNullPID
channel unlock : NonNullPID
LIN_MUTEX = lock?pid -> unlock.pid -> LIN_MUTEX
LIN_SPEC = MUTEX::LIN_SPEC

assert LIN_MUTEX [F= LIN_SPEC

-- Stability checks
HIDE = {|begin_claim, end_release|}
TauPriorSPEC = MUTEX::TauPriorSPEC

assert not TauPriorSPEC [T= SYSTEM
assert TauPriorSPEC \ HIDE [F= SYSTEM \ HIDE

-- Correct state checks
-- end_claim should only occur when state = ACTIVE
CHECK_STATE(pid, s) =
    getState.pid?st ->
    if st == s then
        SKIP
    else
        DIV

USER_SAFE(pid) =
    CHECK_STATE(pid, ACTIVE);
    CLAIM(pid);
    CHECK_STATE(pid, ACTIVE);
    RELEASE(pid);
    USER_SAFE(pid)

-- Safe system should not deadlock
SYSTEM_SAFE =
(
    (Q::QUEUE(users) ||| STATE_VAR ||| OWNER)
    [| union(Q::alpha, alphaVAR) |]
    (||| u : users @ USER_SAFE(u))
) \ union(Q::alpha, alphaVAR)

assert SYSTEM_SAFE :[divergence free]

-- Fairness checks
-- Model of fair FIFO behaviour of the mutex
FAIR'(claims, <>) = card(claims) <= card(users) & (
    (
        card(claims) < card(users) & begin_claim?pid : diff(users, claims) ->
        FAIR'(union(claims, {pid}), <>)
    )
    []
    (
        card(claims) > 0 & [] pid : claims @ Q::lin_enqueue.pid.pid ->
        FAIR'(claims, <pid>)
    )
)

FAIR'(claims, <head>^waiting) = card(claims) <= card(users) and length(<head>^waiting) <= card(users) & (
    (
        card(claims) < card(users) & begin_claim?pid : diff(users, claims) ->
        FAIR'(union(claims, {pid}), <head>^waiting)
    )
    []
    (
        card(claims) > 0 & [] pid : diff(claims, set(<head>^waiting)) @ Q::lin_enqueue.pid.pid ->
        FAIR'(claims, <head>^waiting^<pid>)
    )
    []
    (
        end_claim.head ->
        FAIR'(diff(claims, {head}), waiting)
    )
)

FAIR = FAIR'({}, <>)

-- Implementation of system to check for fairness. Exposes
-- different events.
FAIR_IMPL = 
(
    (Q::QUEUE_VIS(users) ||| STATE_VAR ||| OWNER)
    [| union(Q::alpha, alphaVAR) |]
    (||| u : users @ USER(u))
) \ Union({Q::alpha, alphaVAR, {|begin_release, end_release|}})

assert FAIR [F= FAIR_IMPL
